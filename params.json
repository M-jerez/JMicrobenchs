{"note":"Don't delete this file! It's used internally to help with page regeneration.","google":"","body":"\r\n### What is this library for.\r\nThis API is intended for make micro-benchmarks in java and generate nice html reports using [HighChars.com](http://www.highcharts.com/). It doesn't provide advanced profile functions, abnormal data detection or any other. *The main objective was to create a library for generate nice reports showing the performance's chart and the executed code at the same time,* leaving to the programmer the responsibility of the benchmark's reliability.\r\n\r\nSample report: *this is an just an image, the real report is generated in html. [demos web page.] (http://m-jerez.github.com/JMicrobenchs/)*\r\n\r\n![Jmicrobech sample report](https://raw.github.com/M-jerez/JMicrobenchs/master/media/report-sample.png)\r\n\r\n### What's a micro-benchmark. \r\nA micro-benchmark measures and compares the time performance of code portions, this can be useful to make a general image of the code performance, to find bottlenecks on it or unusual slow behavior, nevertheless micro-benchmarks are not intended for software profiling. \r\n\r\nTo get more info about micro-benchmarks can read the following docs:\r\n* [Caliper micro-benchmarks.](https://code.google.com/p/caliper/wiki/JavaMicrobenchmarks)\r\n* [Robust benchmarking by Brent Boyer at ibm.](http://www.ibm.com/developerworks/java/library/j-benchmark1/index.html)\r\n* [Anatomy of a flawed microbenchmark by Brian Goetz at ibm.](http://www.ibm.com/developerworks/java/library/j-jtp02225/index.html)\r\n* [Benchmark tips, oracle HotSpot documentation.](https://wikis.oracle.com/display/HotSpotInternals/MicroBenchmarks) \r\n \r\n### Benchmark phases.\r\nEach benchmark consist of three different phases while the code is executed a number times.\r\n* **Loading:** This is the first phase and is executed only once. It shows useful results only when the program is being rung by the first time and java must load all classes. This phase is intended to measure cold start performance.\r\n* **Warmup:** This phase must be executed many times to let the JVM make some code optimization, [JIT compile](http://en.wikipedia.org/wiki/Just-in-time_compilation), garbage collection, etc... During this phase the performance is not measured and will not generate any report.\r\n* **Profiling:** This is the main phase and it doesn't must be executed many times but enough to calculate a reliable average. Execute this phase more than required could leave to incorrect results as garbage collection or other processes can distort this results.  \r\n  \r\n\r\n## Show me the code.\r\n\r\n### Write the code.\r\nImplement `JMicrobench` and put the code you want to profile inside the the `runBech()` method. To profile one portion of code create a `TimeProfiler` object and call it's function `startCount()` and `stopCount`.\r\n```java\r\n public class FirstTest implements JMicrobench {\r\n\tpublic void runBench() {\r\n\t\tString t1 = \"void\", t2 = \"create HashMap\", t3 = \"HashMap put\";\r\n\t\t\r\n\t\tTimeProfiler tp = Registry.getTimeProfiler(\"FirstTest Performance\");\r\n\t\t\r\n\t\ttp.startCount(t1);\r\n\t\t/* Nothing Executed here */\r\n\t\t/* time consumed by startCount() & stopCount() */\r\n\t\ttp.stopCount(t1);\r\n\r\n\t\t/* Measures the time required to create a HashMap. */\r\n\t\ttp.startCount(t2);\r\n\t\tHashMap<String, String> h = new HashMap<String, String>();\r\n\t\ttp.stopCount(t2);\r\n\r\n\t\t/* Measures the time required for the put operation */\r\n\t\ttp.startCount(t3);\r\n\t\th.put(\"hello\", \"world\");\r\n\t\ttp.stopCount(t3);\t\r\n\t}\r\n}\r\n```\r\nAs you can see it is not possible to create a `TimeProfiler` directly, all TimeProfilers are created/accessed in a static way through the `Registry` class, this class is basically a Map of TimeProfilers objects identified by it's title. This behavior allows to access the same `TimeProfiler` in any point of the executed program. \r\n\r\nAs you can also see one of it's drawbacks is that write this kind of benchmark is too verbose, but this allows to make accurate measures, nested calls etc...\r\n\r\n\r\n### Run the benchmark.\r\nCreate a new instance of your test class, in this case `FirstTest`; create a new `BenchmarkRunner` and run it `run()`;  finally create one of the available `Reports` and write to File or Output.\r\n```java\r\n\tpublic static void main(String[] args) {\t\t\r\n\t\tint profileLoops = 50;\r\n\t\tint warmupLoops = profileLoops * 1000;\r\n\t\t\r\n\t\tFirstTest firstTest = new FirstTest();\r\n\r\n\t\t/* create & run the benchmark */\r\n\t\tnew BenchmarkRunner(warmupLoops, profileLoops,firstTest).run();\r\n\r\n\t\t/* generate the report & write to file */\t\r\n\t\tReportOptions options = new  ReportOptions(\"../test/\");\r\n\t\tnew FullWebReport(options).writeFullWebToFile(\"C:/Users/mjerez/Desktop/report.html\");\r\n\r\n\t}\r\n```\r\n\r\nThe `RenderOptions` object is required to configure the generated report, in this case we only have set the relative path were are the java source files. if the source files \".java\" and the executables \".class\" are in the same directories this path is a void string -> \"\". To see more options go to [javadoc.](http://m-jerez.github.com/JMicrobenchs/doc/mjerez/jmicrobench/reports/ReportOptions.html)\r\n\r\n### Watch results.\r\nThe next image is a snapshot of the html generated from the previous code. One report like this will be generated for each `TimeProfiler` object created during the benchmark execution.\r\n\r\n![FirstTest Jmicrobech report](https://raw.github.com/M-jerez/JMicrobenchs/master/media/report-FirstTest.png)\r\n\r\n\r\n### Where to find more info.\r\n* [JMicrobech web page.](http://m-jerez.github.com/JMicrobenchs/)\r\n* [Javadoc.](http://m-jerez.github.com/JMicrobenchs/doc/)\r\n\r\n","name":"Jmicrobenchs","tagline":"Java micro-benchmarks made easy."}