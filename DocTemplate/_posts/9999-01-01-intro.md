---
layout: default
title: Introduction to JMicrobenchs.
permalink: index.html

headers:
- title: What's this library for?
  id: whats-library-for
- title: Whats a micro benchmark?
  id: whats-micro-benchmark
- title: Benchmark phases.
  id: benchmark-phases
- title: Write the benchmark.
  id: write-benchmark
- title: Run the benchmark.
  id: run-benchmark
- title: The Reports.
  id: the-reports
---

## {{page.title}}


### What is this library for?   {#{{page.headers[0].id}}}
This API is intended for help developers to make micro-benchmarks in java and generate nice html reports using charts from [HighChars.com](http://www.highcharts.com/). This library doesn't provide advanced profiling functions, like data correction, abnormal measures detection or any other. *The main functionality is run benchmarks and generate reports in html displaying performance's charts and info about the executed code.* The developer is responsible for the benchmark's reliability.

**This is an example of the report generated by JMicrobenchs.**

{% include bench1.md   %}


### What's a micro-benchmark?  {#{{page.headers[1].id}}}
A micro-benchmark measures and compares the time performance of code portions, this can be useful to make a general image of the code performance, to find bottlenecks on it or unusual slow behavior, nevertheless micro-benchmarks are not intended for software profiling. 

To get more info about micro-benchmarks can read the following docs:
* [Caliper micro-benchmarks.](https://code.google.com/p/caliper/wiki/JavaMicrobenchmarks)
* [Robust benchmarking by Brent Boyer at ibm.](http://www.ibm.com/developerworks/java/library/j-benchmark1/index.html)
* [Anatomy of a flawed microbenchmark by Brian Goetz at ibm.](http://www.ibm.com/developerworks/java/library/j-jtp02225/index.html)
* [Benchmark tips, oracle HotSpot documentation.](https://wikis.oracle.com/display/HotSpotInternals/MicroBenchmarks) 
 
### Benchmark phases.  {#{{page.headers[2].id}}}
Each benchmark consist of three different phases while the code is executed a number times.
* **Loading:** This is the first phase and is executed only once. It shows useful results only when the program is being rung by the first time and java must load all classes. This phase is intended to measure cold start performance.
* **Warmup:** This phase must be executed many times to let the JVM make some code optimization, [JIT compile](http://en.wikipedia.org/wiki/Just-in-time_compilation), garbage collection, etc... During this phase the performance is not measured and will not generate any report.
* **Profiling:** This is the main phase and it doesn't must be executed many times but enough to calculate a reliable average. Execute this phase more than required could leave to incorrect results as garbage collection or other processes can distort this results.  

### Write the benchmark.  {#{{page.headers[3].id}}}
Implement `JMicrobench` and put the code you want to profile inside the the `runBech()` method. To profile one portion of code create a `TimeProfiler` object and call it's function `startCount()` and `stopCount`.


~~~ java
public class FirstTest implements JMicrobench {
	public void runBench() {
		String t1 = "void", t2 = "create HashMap", t3 = "HashMap put";
		
		TimeProfiler tp = Registry.getTimeProfiler("FirstTest Performance");
		
		tp.startCount(t1);
		/* Nothing Executed here */
		/* time consumed by startCount() & stopCount() */
		tp.stopCount(t1);

		/* Measures the time required to create a HashMap. */
		tp.startCount(t2);
		HashMap<String, String> h = new HashMap<String, String>();
		tp.stopCount(t2);

		/* Measures the time required for the put operation */
		tp.startCount(t3);
		h.put("hello", "world");
		tp.stopCount(t3);	
	}
}
~~~


As you can see it is not possible to create a `TimeProfiler` directly, all TimeProfilers are created/accessed in a static way through the `Registry` class, this class is basically a Map of TimeProfilers objects identified by it's title. This behavior allows to access the same `TimeProfiler` in any point of the executed program. 

As you can also see one of it's drawbacks is that write this kind of benchmark is too verbose, but this allows to make accurate measures, nested calls etc...


### Run the benchmark.    {#{{page.headers[4].id}}}
Create a new instance of your test class, in this case `FirstTest`; create a new `BenchmarkRunner` and run it `run()`;  finally create one of the available `Reports` and write to File or Output.
{% highlight java %}
	public static void main(String[] args) {		
		int profileLoops = 50;
		int warmupLoops = profileLoops * 1000;
		
		FirstTest firstTest = new FirstTest();

		/* create & run the benchmark */
		new BenchmarkRunner(warmupLoops, profileLoops,firstTest).run();

		/* generate the report & write to file */	
		ReportOptions options = new  ReportOptions("../test/");
		new FullWebReport(options).writeFullWebToFile("C:/Users/mjerez/Desktop/report.html");

	}
{% endhighlight %}

The `RenderOptions` object is required to configure the generated report, in this case we only have set the relative path were are the java source files. if the source files ".java" and the executables ".class" are in the same directories this path is a void string -> "". To see more options go to [javadoc.](http://m-jerez.github.com/JMicrobenchs/doc/mjerez/jmicrobench/reports/ReportOptions.html)

### The Reports.   {#{{page.headers[5]}}}
The next image is a snapshot of the html generated from the previous code. One report like this will be generated for each `TimeProfiler` object created during the benchmark execution.

![FirstTest Jmicrobech report](https://raw.github.com/M-jerez/JMicrobenchs/master/media/report-FirstTest.png)




